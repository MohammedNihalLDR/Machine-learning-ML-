# -*- coding: utf-8 -*-
"""KNN-Based LoanApproval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hNTQRO3Oc4Hd1tuDPToQmA3GMoVHFn_c
"""

# Importing Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# Load the dataset
file_path = "https://raw.githubusercontent.com/prasertcbs/basic-dataset/refs/heads/master/Loan-Approval-Prediction.csv"
df = pd.read_csv(file_path)
# Display first few rows to understand the structure of the dataset
df.head()

# Removing the 'Loan_ID' column as it is not a useful feature
df.drop(columns=['Loan_ID'], inplace=True)
# Verifying the column is removed
df

df.describe()

#datatypes
df.dtypes

# Step 3: Data Understanding

# Checking dataset information
df_info = df.info()

# Checking statistical summary
df_description = df.describe()

# Identifying missing values
missing_values = df.isnull().sum()

# Visualizing Missing Values per Feature
plt.figure(figsize=(8, 5))
missing_values.plot(kind='bar', color='red')
plt.title("Missing Values per Feature")
plt.xlabel("Features")
plt.ylabel("Number of Missing Values")
plt.xticks(rotation=45)
plt.show()

# Displaying missing values count
missing_values

# Step 4: Handling Missing Values

# Handling Missing Values Numerical columns: Using median because it is less affected by outliers
df['LoanAmount'] = df['LoanAmount'].fillna(df['LoanAmount'].median())
df['Loan_Amount_Term'] = df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].median())

# Handling Missing Values Categorical columns: Using mode because it represents the most common value
df['Credit_History'] = df['Credit_History'].fillna(df['Credit_History'].mode()[0])

# Verifying missing values are handled
df.isnull().sum()

# unique valuve of gender, married, dependents,self_employed
print(df['Gender'].unique())
print(df['Married'].unique())
print(df['Dependents'].unique())
print(df['Self_Employed'].unique())

df["Dependents"]=df["Dependents"].replace("3+","3")
print(df['Dependents'].unique())

#value count of Gender,married,selfemployed,dependent give in print
print(df["Gender"].value_counts())
print(df["Married"].value_counts())
print(df["Self_Employed"].value_counts())
print(df["Dependents"].value_counts())

#fill the null value in Gender="Female",married="No",selfemployed="yes",dependent="3"
df.update(df[["Gender", "Married", "Self_Employed", "Dependents"]].fillna({"Gender": "Female",
                                                                            "Married": "No",
                                                                            "Self_Employed": "Yes",
                                                                            "Dependents": "3"}))
df.isnull().sum()

# unique valuve of gender, married, dependents,self_employed
print(df['Gender'].unique())
print(df['Married'].unique())
print(df['Dependents'].unique())
print(df['Self_Employed'].unique())

df

# Step 5: Removing Duplicate Records
# Dropping duplicate rows from the dataset (if any)
df.drop_duplicates(inplace=True)
df.duplicated().sum()

# Step 6: Final Code for Basic Bar Graphs and Pie Charts
# Setting common figure size and grid lines for all graphs
fig_size = (5, 3)

# 1. Bar Chart - Loan Status Distribution
plt.figure(figsize=(5, 3))
sns.countplot(x='Loan_Status', data=df)
plt.title('Loan Approval Status Distribution')
plt.xlabel('Loan Status (0 = Rejected, 1 = Approved)')
plt.ylabel('Count')
plt.grid(True, linestyle='--', linewidth=0.5)
plt.show()

# 2. Bar Chart - Gender Distribution
plt.figure(figsize=fig_size)
sns.countplot(x='Gender', data=df)
plt.title('Gender Distribution of Applicants')
plt.ylabel('Count')
plt.grid(True, linestyle='--', linewidth=0.5)
plt.show()

# 3. Bar Chart - Married Status Distribution
plt.figure(figsize=fig_size)
sns.countplot(x='Married', data=df)
plt.title('Marital Status of Applicants')
plt.xlabel('Married (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.grid(True, linestyle='--', linewidth=0.5)
plt.show()

# 4. Bar Chart - Education Level Distribution
plt.figure(figsize=fig_size)
sns.countplot(x='Education', data=df)
plt.title('Education Level of Applicants')
plt.xlabel('Education (0 = Graduate, 1 = Not Graduate)')
plt.ylabel('Count')
plt.grid(True, linestyle='--', linewidth=0.5)
plt.show()

# 5. Bar Chart - Self Employment Status
plt.figure(figsize=fig_size)
sns.countplot(x='Self_Employed', data=df)
plt.title('Self-Employment Status of Applicants')
plt.xlabel('Self Employed (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.grid(True, linestyle='--', linewidth=0.5)
plt.show()

# 7. Pie Chart - Credit History (Ordinal Data)
plt.figure(figsize=fig_size)
df['Credit_History'].value_counts().plot.pie(
    autopct='%1.1f%%', colors=['lightgreen', 'tomato'],
    startangle=90,
    labels=['Good Credit (1)', 'Bad Credit (0)'])
plt.title('Credit History Distribution')
plt.ylabel('')
plt.show()

# Displaying the remaining correct graphs

# 7. Pie Chart - Gender Distribution in Dataset
plt.figure(figsize=fig_size)
df['Gender'].value_counts().plot.pie(autopct='%1.1f%%', colors=['lightblue', 'pink'], startangle=90, labels=['Male', 'Female'])
plt.title('Gender Distribution in Loan Applications')
plt.ylabel('')
plt.show()

# 7. Pie Chart - Gender Distribution in Dataset
plt.figure(figsize=fig_size)
df['Property_Area'].value_counts().plot.pie(autopct='%1.1f%%', colors=['lightblue', 'pink',"green"], startangle=90,)
plt.title('Gender Distribution in Loan Applications')
plt.ylabel('')
plt.show()

# understanding the distribution of the data
df.describe()

# Creating # Creating distribution plots for all continuous features with different colors
colors = ['blue', 'green', 'red', 'purple', 'orange']

# Define the continuous features
continuous_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History'] # Assuming these are your continuous features

plt.figure(figsize=(12,8))
for i, (feature, color) in enumerate(zip(continuous_features, colors), 1):
    plt.subplot(2, 3, i)
    sns.histplot(df[feature], bins=20, kde=True, color=color)
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

cdf= df.sort_values(by=['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount'], ascending=False)
cdf

# Function to remove outliers using IQR method
def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]

# Applying IQR method to remove outliers and sort
dff = df.sort_values(by=['ApplicantIncome'], ascending=False)#sort
df_cleaned = remove_outliers_iqr(dff, 'ApplicantIncome')#outlier
dff2 = df_cleaned.sort_values(by=['CoapplicantIncome'], ascending=False)#sort
df_cleane = remove_outliers_iqr(dff2, 'CoapplicantIncome')#outlier
dff3 = df_cleane.sort_values(by=['LoanAmount'], ascending=False)#sort
df_clean = remove_outliers_iqr(dff3, 'LoanAmount')#outlier

# Check the new shape of the cleaned dataset
print(f"Original dataset shape: {df.shape}")
print(f"Cleaned dataset shape: {df_cleaned.shape}")

# Display first few rows of cleaned data
df_clean.head()

# Creating # Creating distribution plots for all continuous features with different colors
colors = ['blue', 'green', 'red', 'purple', 'orange']

# Define the continuous features
continuous_features = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History'] # Assuming these are your continuous features

plt.figure(figsize=(12,8))
for i, (feature, color) in enumerate(zip(continuous_features, colors), 1):
    plt.subplot(2, 3, i)
    sns.histplot(df_clean[feature], bins=20, kde=True, color=color)
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

df_clean.describe()

# Function to keep only 90% of data
def keep_90_percent(data, column):
    lower_bound = data[column].quantile(0.05)  # 5th percentile
    upper_bound = data[column].quantile(0.95)  # 95th percentile
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]
dfg=keep_90_percent(df_clean, 'ApplicantIncome')
dfg.describe()

import seaborn as sns
import matplotlib.pyplot as plt

# Compute correlation with the target variable (Loan_Status)
corr_with_target = df_clean.corr()['Loan_Status'].drop('Loan_Status')  # Drop self-correlation

# Plot a bar chart for correlation values
plt.figure(figsize=(8, 4))
ax = sns.barplot(x=corr_with_target.index, y=corr_with_target.values, hue=corr_with_target.index, palette='coolwarm', legend=False)

# Annotate bars with correlation values
for p in ax.patches:
    ax.annotate(f"{p.get_height():.2f}", (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=12, color='black')

# Titles and Labels (Removed Emoji)
plt.xticks(rotation=45)
plt.title("Feature Correlation with Loan_Status", fontsize=14)
plt.ylabel("Correlation Coefficient")
plt.xlabel("Features")
plt.show()

from sklearn.preprocessing import LabelEncoder

# Ensure df_clean is a deep copy to avoid the SettingWithCopyWarning
df_clean = df_clean.copy()

# List of categorical columns
categorical_columns = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status', 'Dependents']

# Fill missing values with mode before encoding
for col in categorical_columns:
    df_clean.loc[:, col] = df_clean[col].fillna(df_clean[col].mode()[0])

# Apply Label Encoding safely using .loc
label_encoders = {}  # Store encoders for inverse transformation

for col in categorical_columns:
    le = LabelEncoder()
    df_clean.loc[:, col] = le.fit_transform(df_clean[col])  # Apply encoding using .loc
    label_encoders[col] = le  # Save encoder for later use

# Display first few rows after encoding
df_clean

#uniqiue values
df_clean["Dependents"].unique()

print(df_clean['Loan_Status'].unique())
print(df_clean['Loan_Status'].dtype)

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df_clean['Loan_Status'] = le.fit_transform(df_clean['Loan_Status'])  # Converts 'Y' to 1, 'N' to 0

df_clean.nunique()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1ï¸âƒ£ Define the target variable (y) and features (X)
X = df_clean.drop(columns=['Loan_Status'])  # Features
y = df_clean['Loan_Status']  # Target variable

# 2ï¸âƒ£ Split the dataset into training & testing sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 3ï¸âƒ£ Standardize the numerical features (Scale for better model performance)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit & transform training data
X_test_scaled = scaler.transform(X_test)  # Transform test data

# 4ï¸âƒ£ Initialize & Train the Model (KNN)
model = KNeighborsClassifier(n_neighbors=12)  # You can change 'n_neighbors' for tuning
model.fit(X_train_scaled, y_train)  # Model fitting

# 5ï¸âƒ£ Make Predictions
y_pred = model.predict(X_test_scaled)

# 6ï¸âƒ£ Model Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"âœ… Model Accuracy: {accuracy:.4f}")  # Print accuracy
print("ðŸ”¹ Classification Report:\n", classification_report(y_test, y_pred))

# 7ï¸âƒ£ Predict on Training & Testing Data
y_train_pred = model.predict(X_train_scaled)  # Training predictions
y_test_pred = model.predict(X_test_scaled)  # Testing predictions

# 8ï¸âƒ£ Calculate Training & Testing Accuracy
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)

# 9ï¸âƒ£ Display Results
print(f"ðŸŽ¯ Training Accuracy: {train_accuracy:.4f}")
print(f"ðŸ§ª Testing Accuracy: {test_accuracy:.4f}")

# ðŸ”¹ Display Classification Report for Testing Data
print("ðŸ”¹ Classification Report (Testing Data):\n", classification_report(y_test, y_test_pred))

from sklearn.neighbors import KNeighborsClassifier

train_accuracies = []
test_accuracies = []
k_values = range(1, 100, 2)  # Test odd K values from 1 to 29

for k in k_values:
    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train_scaled, y_train)

    train_acc = accuracy_score(y_train, model.predict(X_train_scaled))
    test_acc = accuracy_score(y_test, model.predict(X_test_scaled))

    train_accuracies.append(train_acc)
    test_accuracies.append(test_acc)

# Plot Training vs Testing Accuracy
plt.plot(k_values, train_accuracies, label='Training Accuracy', marker='o')
plt.plot(k_values, test_accuracies, label='Testing Accuracy', marker='s')
plt.xlabel('K Value')
plt.ylabel('Accuracy')
plt.title('KNN - Accuracy vs K')
plt.legend()
plt.grid()
plt.show()

# Best K value
best_K = k_values[np.argmax(test_accuracies)]
print(f"âœ… Best K for KNN: {best_K}")
print(f"ðŸŽ¯ Highest Training Accuracy: {max(train_accuracies):.4f}")
print(f"ðŸ§ª Highest Testing Accuracy: {max(test_accuracies):.4f}")

import numpy as np
import ipywidgets as widgets
from IPython.display import display, clear_output
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier

# Mapping dictionary for categorical inputs
gender_mapping = {"Female": 0, "Male": 1}
married_mapping = {"No": 0, "Yes": 1}
dependents_mapping = {"None": 0, "One": 1, "Two": 2, "Three": 3,"Four": 4}
education_mapping = {"Graduate": 0, "Not Graduate": 1}
self_employed_mapping = {"No": 0, "Yes": 1}
credit_history_mapping = {"Bad": 0, "Good": 1}
property_area_mapping = {"Rural": 0, "Semiurban": 1, "Urban": 2}

# Dropdowns for categorical variables (with spacing)
gender_dropdown = widgets.Dropdown(options=gender_mapping.keys(), description="Gender:  ", style={'description_width': '150px'})
married_dropdown = widgets.Dropdown(options=married_mapping.keys(), description="Married:  ", style={'description_width': '150px'})
dependents_dropdown = widgets.Dropdown(options=dependents_mapping.keys(), description="Dependents:  ", style={'description_width': '150px'})
education_dropdown = widgets.Dropdown(options=education_mapping.keys(), description="Education:  ", style={'description_width': '150px'})
self_employed_dropdown = widgets.Dropdown(options=self_employed_mapping.keys(), description="Self Employed:  ", style={'description_width': '150px'})
credit_history_dropdown = widgets.Dropdown(options=credit_history_mapping.keys(), description="Credit History:  ", style={'description_width': '150px'})
property_area_dropdown = widgets.Dropdown(options=property_area_mapping.keys(), description="Property Area:  ", style={'description_width': '150px'})

# Numeric inputs with labels
applicant_income_input = widgets.FloatText(description="Applicant Income:  ", style={'description_width': '150px'})
coapplicant_income_input = widgets.FloatText(description="Coapplicant Income:  ", style={'description_width': '150px'})
loan_amount_input = widgets.FloatText(description="Loan Amount:  ", style={'description_width': '150px'})
loan_term_input = widgets.FloatText(description="Loan Term (days):  ", style={'description_width': '150px'})

# Prediction button
predict_button = widgets.Button(description="Check Loan Approval", button_style='success')
output_label = widgets.Output()

# Function to predict loan approval
def predict_loan_approval(b):
    with output_label:
        clear_output()

        # Convert dropdown selections to numeric values
        user_data = np.array([[gender_mapping[gender_dropdown.value],
                               married_mapping[married_dropdown.value],
                               dependents_mapping[dependents_dropdown.value],
                               education_mapping[education_dropdown.value],
                               self_employed_mapping[self_employed_dropdown.value],
                               applicant_income_input.value,
                               coapplicant_income_input.value,
                               loan_amount_input.value,
                               loan_term_input.value,
                               credit_history_mapping[credit_history_dropdown.value],
                               property_area_mapping[property_area_dropdown.value]]])

        # Standardizing the input
        user_data_scaled = scaler.transform(user_data)  # No error now!

        # Making prediction
        prediction = model.predict(user_data_scaled)

        # Display result
        result_text = "ðŸŽ‰ Loan Approved! âœ…" if prediction[0] == 1 else "ðŸš« Loan Not Approved âŒ"
        print(result_text)

# Link button click to function
predict_button.on_click(predict_loan_approval)

# Layout for better spacing
form_layout = widgets.VBox([
    gender_dropdown, married_dropdown, dependents_dropdown, education_dropdown,
    self_employed_dropdown, credit_history_dropdown, property_area_dropdown,
    applicant_income_input, coapplicant_income_input, loan_amount_input, loan_term_input,
    widgets.HTML("<br>"),  # Extra spacing
    predict_button, output_label
])

# Display form
display(form_layout)